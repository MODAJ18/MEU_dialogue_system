{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d45e546",
   "metadata": {},
   "source": [
    "# `1) Task-Oriented NLU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42db99",
   "metadata": {},
   "source": [
    "`TASK` -> Smart Home Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34787298",
   "metadata": {},
   "source": [
    "### `1.1) Imports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08383088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3068b8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23132 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = None\n",
    "\n",
    "fs_df = pd.read_csv('Data/fluent_speech_commands_dataset/data/train_data.csv', delimiter=',', nrows = nRowsRead)\n",
    "fs_df.dataframeName = 'train_data.csv'\n",
    "nRow, nCol = fs_df.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115d4d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>speakerId</th>\n",
       "      <th>transcription</th>\n",
       "      <th>action</th>\n",
       "      <th>object</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wavs/speakers/2BqVo8kVB2Skwgyb/0a3129c0-4474-1...</td>\n",
       "      <td>2BqVo8kVB2Skwgyb</td>\n",
       "      <td>Change language</td>\n",
       "      <td>change language</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wavs/speakers/2BqVo8kVB2Skwgyb/0ee42a80-4474-1...</td>\n",
       "      <td>2BqVo8kVB2Skwgyb</td>\n",
       "      <td>Resume</td>\n",
       "      <td>activate</td>\n",
       "      <td>music</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wavs/speakers/2BqVo8kVB2Skwgyb/144d5be0-4474-1...</td>\n",
       "      <td>2BqVo8kVB2Skwgyb</td>\n",
       "      <td>Turn the lights on</td>\n",
       "      <td>activate</td>\n",
       "      <td>lights</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wavs/speakers/2BqVo8kVB2Skwgyb/1811b6e0-4474-1...</td>\n",
       "      <td>2BqVo8kVB2Skwgyb</td>\n",
       "      <td>Switch on the lights</td>\n",
       "      <td>activate</td>\n",
       "      <td>lights</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wavs/speakers/2BqVo8kVB2Skwgyb/1d9f3920-4474-1...</td>\n",
       "      <td>2BqVo8kVB2Skwgyb</td>\n",
       "      <td>Switch off the lights</td>\n",
       "      <td>deactivate</td>\n",
       "      <td>lights</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               path  \\\n",
       "0           0  wavs/speakers/2BqVo8kVB2Skwgyb/0a3129c0-4474-1...   \n",
       "1           1  wavs/speakers/2BqVo8kVB2Skwgyb/0ee42a80-4474-1...   \n",
       "2           2  wavs/speakers/2BqVo8kVB2Skwgyb/144d5be0-4474-1...   \n",
       "3           3  wavs/speakers/2BqVo8kVB2Skwgyb/1811b6e0-4474-1...   \n",
       "4           4  wavs/speakers/2BqVo8kVB2Skwgyb/1d9f3920-4474-1...   \n",
       "\n",
       "          speakerId          transcription           action  object location  \n",
       "0  2BqVo8kVB2Skwgyb        Change language  change language    none     none  \n",
       "1  2BqVo8kVB2Skwgyb                 Resume         activate   music     none  \n",
       "2  2BqVo8kVB2Skwgyb     Turn the lights on         activate  lights     none  \n",
       "3  2BqVo8kVB2Skwgyb   Switch on the lights         activate  lights     none  \n",
       "4  2BqVo8kVB2Skwgyb  Switch off the lights       deactivate  lights     none  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3193c",
   "metadata": {},
   "source": [
    "**NLU:** `transcription` ---> `intent` + `object` + `location`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6344ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     location\n",
       "volume     none        4395\n",
       "heat       washroom    2565\n",
       "           none        2105\n",
       "lights     washroom    1397\n",
       "           kitchen     1326\n",
       "music      none        1326\n",
       "heat       bedroom     1303\n",
       "           kitchen     1282\n",
       "lights     bedroom     1109\n",
       "none       none         994\n",
       "lights     none         962\n",
       "lamp       none         792\n",
       "newspaper  none         551\n",
       "socks      none         538\n",
       "shoes      none         536\n",
       "juice      none         466\n",
       "Chinese    none         449\n",
       "English    none         349\n",
       "Korean     none         345\n",
       "German     none         342\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_df[[\"object\", \"location\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642f174",
   "metadata": {},
   "source": [
    "### `1.2) Data Exploration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b6602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fs_df[[\"transcription\", \"action\", \"object\", \"location\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e9c540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>action</th>\n",
       "      <th>object</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Change language</td>\n",
       "      <td>change language</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Resume</td>\n",
       "      <td>activate</td>\n",
       "      <td>music</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Turn the lights on</td>\n",
       "      <td>activate</td>\n",
       "      <td>lights</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Switch on the lights</td>\n",
       "      <td>activate</td>\n",
       "      <td>lights</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Switch off the lights</td>\n",
       "      <td>deactivate</td>\n",
       "      <td>lights</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           transcription           action  object location\n",
       "0        Change language  change language    none     none\n",
       "1                 Resume         activate   music     none\n",
       "2     Turn the lights on         activate  lights     none\n",
       "3   Switch on the lights         activate  lights     none\n",
       "4  Switch off the lights       deactivate  lights     none"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "969485d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "increase           5953\n",
       "decrease           5697\n",
       "activate           3822\n",
       "deactivate         3090\n",
       "change language    2479\n",
       "bring              2091\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"action\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855ddbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "heat         7255\n",
       "lights       4794\n",
       "volume       4395\n",
       "music        1326\n",
       "none          994\n",
       "lamp          792\n",
       "newspaper     551\n",
       "socks         538\n",
       "shoes         536\n",
       "juice         466\n",
       "Chinese       449\n",
       "English       349\n",
       "Korean        345\n",
       "German        342\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"object\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab49134e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none        14150\n",
       "washroom     3962\n",
       "kitchen      2608\n",
       "bedroom      2412\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"location\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b8fe3",
   "metadata": {},
   "source": [
    "### `1.3) Preprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a830c6",
   "metadata": {},
   "source": [
    "#### 1.3.1- text correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96cc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b8e7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'turn on the lights!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"turn on the lights!\"\n",
    "txt = str(TextBlob(txt).correct())\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa732055",
   "metadata": {},
   "source": [
    "#### 1.3.2- lowecase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fb59fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for honor'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"For HoNOR\"\n",
    "txt = txt.lower().strip()\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d393d",
   "metadata": {},
   "source": [
    "#### 1.3.3- remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cdceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say hi neighbor'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "txt = \"say hi to the neighbor\"\n",
    "txt = remove_stopwords(txt)\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375cfdb",
   "metadata": {},
   "source": [
    "#### 1.3.4- lemmentization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c420aaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahmad be work on project'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV} \n",
    "# Pos tag, used Noun, Verb, Adjective and Adverb\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "#     print(pos_tagged_text)\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "txt = \"Ahmad is working on projects\"\n",
    "lemmatize_words(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002138e",
   "metadata": {},
   "source": [
    "#### 1.3.5 apply text-cleaning methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7852fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    text = text.lower().strip()\n",
    "    text = str(TextBlob(text).correct())\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35198bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\AppData\\Local\\Temp\\ipykernel_88488\\3974121563.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"transcription_clean\"] = data[\"transcription\"].apply(lambda x: text_clean(x))\n"
     ]
    }
   ],
   "source": [
    "data[\"transcription_clean\"] = data[\"transcription\"].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3b1ea",
   "metadata": {},
   "source": [
    "#### `1.3.6) TF-IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "747daa7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3326abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f494615",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tfidf_vectorizer.fit_transform(data[\"transcription_clean\"] )\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# tfidf_df = pd.DataFrame(values.toarray(), columns = tfidf_feature_names)\n",
    "# tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79adadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb78f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# tfidf_df = pd.DataFrame(values.toarray(), columns = tfidf_feature_names)\n",
    "# tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60a3cf",
   "metadata": {},
   "source": [
    "#### `1.3.7) encoding categorical features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383b4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "460156ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = values\n",
    "\n",
    "le_action = preprocessing.LabelEncoder()\n",
    "le_object = preprocessing.LabelEncoder()\n",
    "le_location = preprocessing.LabelEncoder()\n",
    "y_action, y_object, y_location = le_action.fit_transform(data[\"action\"]),\\\n",
    "                                                  le_object.fit_transform(data[\"object\"]),\\\n",
    "                                                  le_location.fit_transform(data[\"location\"])\n",
    "\n",
    "# le_action.inverse_transform(y_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e255157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\AppData\\Local\\Temp\\ipykernel_88488\\1416426773.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"state\"] = data[\"action\"] + \"-\" + data[\"object\"] + \"-\" + data[\"location\"]\n"
     ]
    }
   ],
   "source": [
    "data[\"state\"] = data[\"action\"] + \"-\" + data[\"object\"] + \"-\" + data[\"location\"]\n",
    "y_state = le.fit_transform(data[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddded068",
   "metadata": {},
   "source": [
    "### `1.4) Training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "970ae1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047ad1c",
   "metadata": {},
   "source": [
    "#### 1.4.1 choosing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ddd4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "MultinomialNB_model = MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
    "LogReg_model = LogisticRegression()\n",
    "SGDClassifier_model = SGDClassifier(class_weight='balanced', penalty='l1')\n",
    "\n",
    "models = [MultinomialNB_model, LogReg_model, SGDClassifier_model]\n",
    "model_names = ['MultinomialNB', 'LogisticRegression', 'SGDClassifier']\n",
    "\n",
    "def train_and_evaluate(X, y, label_col='y'):\n",
    "    def train_models(X_tr, X_te, y_tr, y_te):\n",
    "        for i, model in enumerate(models):\n",
    "            print(f\"Model: {model_names[i]}\")\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_te)\n",
    "            print('accuracy %s' % accuracy_score(y_te, y_pred))\n",
    "            print()\n",
    "    print(\"----training on data with labels for column: {}----\".format(label_col))\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=110 , stratify=y)\n",
    "    train_models(X_train, X_test, y_train, y_test)\n",
    "    print(\"-\" * 100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c19b0d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----training on data with labels for column: action----\n",
      "Model: MultinomialNB\n",
      "accuracy 0.7516749513723795\n",
      "\n",
      "Model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\test-venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7516749513723795\n",
      "\n",
      "Model: SGDClassifier\n",
      "accuracy 0.7592392478928031\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----training on data with labels for column: object----\n",
      "Model: MultinomialNB\n",
      "accuracy 0.988329371082775\n",
      "\n",
      "Model: LogisticRegression\n",
      "accuracy 1.0\n",
      "\n",
      "Model: SGDClassifier\n",
      "accuracy 1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----training on data with labels for column: location----\n",
      "Model: MultinomialNB\n",
      "accuracy 0.9394856278366112\n",
      "\n",
      "Model: LogisticRegression\n",
      "accuracy 1.0\n",
      "\n",
      "Model: SGDClassifier\n",
      "accuracy 1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----training on data with labels for column: state----\n",
      "Model: MultinomialNB\n",
      "accuracy 0.7516749513723795\n",
      "\n",
      "Model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\test-venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7609682299546142\n",
      "\n",
      "Model: SGDClassifier\n",
      "accuracy 0.7663712988977739\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training + simple evaluation\n",
    "train_and_evaluate(X, y_action, \"action\")\n",
    "train_and_evaluate(X, y_object, \"object\")\n",
    "train_and_evaluate(X, y_location, \"location\")\n",
    "train_and_evaluate(X, y_state, \"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d566af",
   "metadata": {},
   "source": [
    "`Evaluation`: Logisitic Regression is the best model in terms of its simplicity and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a05e91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\test-venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7516749513723795\n",
      "\n",
      "column: object\n",
      "accuracy 1.0\n",
      "\n",
      "column: location\n",
      "accuracy 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Final Model\n",
    "y_intents = [y_action, y_object, y_location]\n",
    "y_intents = {\"action\": y_action, \"object\": y_object, \"location\": y_location}\n",
    "les = {\"action\": le_action, \"object\": le_object, \"location\": le_location}\n",
    "intent_models = {}\n",
    "X = values\n",
    "classes = []\n",
    "\n",
    "for intent, y_intent in y_intents.items():\n",
    "    print(\"column: {}\".format(intent))\n",
    "    LogReg_model = LogisticRegression()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_intent, test_size=0.2, random_state=110 , stratify=y_intent)\n",
    "    LogReg_model.fit(X_train, y_train)\n",
    "    y_pred = LogReg_model.predict(X_test)\n",
    "    intent_models[intent] = LogReg_model\n",
    "    print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab0fdb",
   "metadata": {},
   "source": [
    "### `1.5) Applying Model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de78e3",
   "metadata": {},
   "source": [
    "the model trained (logistic regression) is going to be applied in transcripted text for implementation and testing purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8135540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'activate', 'object': 'lights', 'location': 'none'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply intent classification\n",
    "transcript_eg = \"Switch on the lights\"\n",
    "\n",
    "def get_intents(transcript):\n",
    "    transcript = tfidf_vectorizer.transform([transcript])\n",
    "    action_pred = les[\"action\"].inverse_transform(intent_models[\"action\"].predict(transcript))[0]\n",
    "    object_pred = les[\"object\"].inverse_transform(intent_models[\"object\"].predict(transcript))[0]\n",
    "    location_pred = les[\"location\"].inverse_transform(intent_models[\"location\"].predict(transcript))[0]\n",
    "    \n",
    "    state_exists, response = verify_intent(action_pred, object_pred, location_pred)\n",
    "    if state_exists:\n",
    "        return {'action': action_pred, 'object': object_pred, 'location': location_pred}\n",
    "    else:\n",
    "        return response\n",
    "        \n",
    "\n",
    "get_intents(transcript_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ffbf031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'increase', 'object': 'volume', 'location': 'none'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_eg = \"open google\"\n",
    "get_intents(transcript_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "612ae0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_intent(action_inp, object_inp, location_inp):\n",
    "    allowed_actions = {  'music': ['activate', 'deactivate'],\n",
    "                         'lights': ['activate', 'deactivate'],\n",
    "                         'volume': ['increase', 'decrease'],\n",
    "                         'heat': ['increase', 'decrease'],\n",
    "                         'lamp': ['activate', 'deactivate'],\n",
    "                         'newspaper': ['bring'],\n",
    "                         'juice': ['bring'],\n",
    "                         'socks': ['bring'],\n",
    "                         'chinese': ['change language'],\n",
    "                         'korean': ['change language'],\n",
    "                         'english': ['change language'],\n",
    "                         'german': ['change language'],\n",
    "                         'shoes': ['bring']}\n",
    "    allowed_rooms = {'music': [\"none\"],\n",
    "                     'lights': [\"none\", \"bedroom\", \"washroom\", \"kitchen\"],\n",
    "                     'volume': [\"none\"],\n",
    "                     'heat': [\"none\", \"bedroom\", \"washroom\", \"kitchen\"],\n",
    "                     'lamp': [\"none\"],\n",
    "                     'newspaper': [\"none\"],\n",
    "                     'juice': [\"none\"],\n",
    "                     'socks': [\"none\"],\n",
    "                     'chinese': [\"none\"],\n",
    "                     'korean': [\"none\"],\n",
    "                     'english': [\"none\"],\n",
    "                     'german': [\"none\"],\n",
    "                     'shoes': [\"none\"]}\n",
    "    \n",
    "    if object_inp in allowed_actions.keys():\n",
    "        if action_inp in allowed_actions[object_inp]:\n",
    "            if location_inp in allowed_rooms[object_inp]:\n",
    "                return True, {\"action\": action_inp, \"object\": object_inp, \"location\": location_inp}\n",
    "            else:\n",
    "                return False, \"wrong room specification for '{} {}'\".format(action_inp, object_inp)  \n",
    "        else:\n",
    "            return False, \"wrong action for object {}\".format(object_inp)\n",
    "    else:\n",
    "        return False, \"none of the defined objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b14256a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "wrong room specification for 'decrease volume'\n"
     ]
    }
   ],
   "source": [
    "state_exists, response = verify_intent(action_inp=\"decrease\", object_inp=\"volume\", location_inp=\"bedroom\")\n",
    "print(state_exists)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a864c2",
   "metadata": {},
   "source": [
    "# `2) Non-Task Oriented NLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64c4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993acba9",
   "metadata": {},
   "source": [
    "### `2.1) Zero-Shot Classification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d68e9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dialog_manager.states import allowed_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e3a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_labels = [\"conversational\", \"question\", \"command\"]\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa48d47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'define computers',\n",
       " 'labels': ['command', 'question', 'conversational'],\n",
       " 'scores': [0.5320936441421509, 0.361875057220459, 0.10603131353855133]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discourse analysis\n",
    "\n",
    "query_test = \"define computers\"\n",
    "zsc_result = classifier(query_test, discourse_labels)\n",
    "zsc_labels = zsc_result[\"labels\"]  \n",
    "zsc_scores = zsc_result[\"scores\"]\n",
    "zsc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f6460",
   "metadata": {},
   "source": [
    "`Note` we use 'zsc_labels' instead of 'classifier' list because the order of classes used might change after using classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2129f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object and action classification\n",
    "\n",
    "# query_test = \"change language to germany\"\n",
    "# object_labels = ['other', 'music', 'lights', 'volume', 'heat', 'lamp', 'newspaper', 'juice', 'socks',\n",
    "#             'chinese', 'korean', 'english', 'german', 'shoes']\n",
    "# action_labels = ['change language', 'activate', 'deactivate', 'increase', 'decrease', 'bring']\n",
    "\n",
    "# zsc_result = classifier(query_test, object_labels)\n",
    "# zsc_labels = zsc_result[\"labels\"]  \n",
    "# zsc_scores = zsc_result[\"scores\"]\n",
    "# object_chosen = zsc_labels[0]\n",
    "\n",
    "# zsc_result = classifier(query_test, action_labels)\n",
    "# zsc_labels = zsc_result[\"labels\"]  \n",
    "# zsc_scores = zsc_result[\"scores\"]\n",
    "# action_chosen = zsc_labels[0]\n",
    "\n",
    "# print(\"object chosen '{}'\".format(object_chosen))\n",
    "# print(\"action chosen '{}'\".format(action_chosen))\n",
    "\n",
    "# items_in_house = list(allowed_actions.keys())\n",
    "# if object_chosen in items_in_house:\n",
    "#     if action_chosen in allowed_actions[object_chosen]:\n",
    "#         print(\"do action in smarthome\")\n",
    "#     else:\n",
    "#         print(\"don't do action in smarthome\")\n",
    "# # print(\"object\", zsc_result)\n",
    "# # print()\n",
    "# # print(\"action\", zsc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f09ed",
   "metadata": {},
   "source": [
    "### `2.2) Conversational`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55d19f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "converser = pipeline(\"conversational\",\n",
    "                      model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3292ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a054233",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test = \"how are you?\"\n",
    "conversation = Conversation(query_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a903fdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I am doing well, thank you. How are you doing? I hope you are as well.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = converser(conversation).generated_responses[-1]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c88d79d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversation input is to long (61), trimming it to (60 - 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm doing well. Thank you for asking. What do you like to do for fun?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.mark_processed()\n",
    "conversation.append_response(response)\n",
    "conversation.add_user_input(\"how are you?\")\n",
    "response = converser(conversation).generated_responses[-1]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847a010",
   "metadata": {},
   "source": [
    "### `2.3) Rephrasing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrot import Parrot\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "''' \n",
    "uncomment to get reproducable paraphrase generations\n",
    "def random_state(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "random_state(1234)\n",
    "'''\n",
    "\n",
    "#Init models (make sure you init ONLY once if you integrate this to your code)\n",
    "parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\"Can you recommed some upscale restaurants in Newyork?\",\n",
    "           \"What are the famous places we should not miss in Russia?\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "    print(\"-\"*100)\n",
    "    print(\"Input_phrase: \", phrase)\n",
    "    print(\"-\"*100)\n",
    "    para_phrases = parrot.augment(input_phrase=phrase)\n",
    "    for para_phrase in para_phrases:\n",
    "        print(para_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fb372c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rephraser = pipeline(\"text2text-generation\",\n",
    "                      model=\"prithivida/parrot_paraphraser_on_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c18d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What do you think of the recent events in new yourk?'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_test = \"what do you think of the recent events that transpired in new yourk?\"\n",
    "\n",
    "rephraser(query_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e891",
   "metadata": {},
   "source": [
    "### `2.4) story telling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "354093f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8bba0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<BOS> <superhero> Batman and Robin, along with Professor Charles Xavier\\'s X-Men, raid an underground facility that holds the mutant \"Quicksilver\". The group includes the superpowered Jean Grey, Iceman, Scarlet Witch, Magik, Sabret'}]\n"
     ]
    }
   ],
   "source": [
    "story_gen = pipeline(\"text-generation\", \"pranavpsv/gpt2-genre-story-generator\")\n",
    "print(story_gen(\"<BOS> <superhero> Batman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3937bf",
   "metadata": {},
   "source": [
    "### `2.5) Question Answering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "20139c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"what is the best pizza in the world by Patrick Bateman\"\n",
    "sentence = sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ca739",
   "metadata": {},
   "source": [
    "#### `2.5.1) automatic keyword extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "89a88c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1d8d93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_tokenizer = AutoTokenizer.from_pretrained(\"yanekyuk/bert-uncased-keyword-extractor\")\n",
    "keyword_extractor = pipeline(\"token-classification\", model=\"yanekyuk/bert-uncased-keyword-extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0a5fc6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pizza', 'patrick bateman']\n"
     ]
    }
   ],
   "source": [
    "keywords_info = keyword_extractor(sentence)\n",
    "keywords = list(map(lambda x: (x['entity'], x[\"word\"]), keywords_info))\n",
    "\n",
    "keywords_processed = []\n",
    "for key in keywords:\n",
    "    name = key[1]\n",
    "    if 'I' in key[0]:\n",
    "        if \"##\" in name:\n",
    "            keywords_processed[-1] = keywords_processed[-1] + \"{}\".format(name).replace(\"#\", \"\")\n",
    "        else:\n",
    "            keywords_processed[-1] = keywords_processed[-1] + \" {}\".format(name)\n",
    "    else:\n",
    "        keywords_processed.append(name)\n",
    "print(keywords_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbaf48",
   "metadata": {},
   "source": [
    "#### `2.5.2) POS tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "db8ea1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\modaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "387f2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokenized_word, tagset='universal')\n",
    "\n",
    "adjectives = [i[0] for i in pos_tags if i[1] == \"ADJ\"]\n",
    "nouns = [i[0] for i in pos_tags if i[1] == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0e098b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pizza', 'world', 'bateman'], ['best', 'patrick'])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns, adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9962f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if best in 'pizza''\n",
      "if best in 'patrick bateman''\n",
      "if patrick in 'pizza''\n",
      "if patrick in 'patrick bateman''\n",
      "['best']\n",
      "if pizza in 'pizza''\n",
      "if world in 'pizza''\n",
      "if world in 'patrick bateman''\n",
      "if bateman in 'pizza''\n",
      "if bateman in 'patrick bateman''\n",
      "['best', 'world']\n"
     ]
    }
   ],
   "source": [
    "new_keywords_processed = []\n",
    "\n",
    "for adj_i in adjectives:\n",
    "    matched = False\n",
    "    for keyword_i in keywords_processed: \n",
    "        print(\"if {} in '{}''\".format(adj_i, keyword_i))\n",
    "        if adj_i in keyword_i:\n",
    "            matched = True\n",
    "            break\n",
    "            \n",
    "    if not matched:\n",
    "        new_keywords_processed.append(adj_i)\n",
    "print(new_keywords_processed)\n",
    "\n",
    "for noun_i in nouns:\n",
    "    matched = False\n",
    "    for keyword_i in keywords_processed: \n",
    "        print(\"if {} in '{}''\".format(noun_i, keyword_i))\n",
    "        if noun_i in keyword_i:\n",
    "            matched = True\n",
    "            break\n",
    "            \n",
    "    if not matched:\n",
    "        new_keywords_processed.append(noun_i)\n",
    "print(new_keywords_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b1f83097",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_processed.extend(new_keywords_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "305fc3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'patrick bateman', 'best', 'world']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe393161",
   "metadata": {},
   "source": [
    "#### `2.5.3) NER tags` (Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5158a82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\modaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\modaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\modaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import ne_chunk\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2590ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,536.0,120.0\" width=\"536px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"8.95522%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">what</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.47761%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.46269%\" x=\"8.95522%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.6866%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.46269%\" x=\"16.4179%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.1493%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.95522%\" x=\"23.8806%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">best</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.3582%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.4478%\" x=\"32.8358%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pizza</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.0597%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.97015%\" x=\"43.2836%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.2687%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.46269%\" x=\"49.2537%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.9851%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.4478%\" x=\"56.7164%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">world</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.9403%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.97015%\" x=\"67.1642%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.1493%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.4328%\" x=\"73.1343%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">patrick</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.8507%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.4328%\" x=\"86.5672%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">bateman</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.2836%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('pizza', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('by', 'IN'), ('patrick', 'JJ'), ('bateman', 'NN')])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token = word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(text_token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk\n",
    "# chunk.pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d02e35",
   "metadata": {},
   "source": [
    "`note`: we can't use NER, due to the nature of speech recogntion not being case-sensitive in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabc719",
   "metadata": {},
   "source": [
    "#### `2.5.4) QA model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "01413601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "eee5d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "fa42c0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'List of The Good Fight episodes'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_reult = wikipedia.search(query)[0]\n",
    "search_reult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d315e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = wikipedia.summary(\"what is \" + \"google\", sentences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7b7ec393",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is fighting good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "841ec8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Good Fight is an American legal drama produced for CBS's streaming service CBS All Access (later Paramount+). It is the platform's first original scripted series. The series, created by Robert King, Michelle King, and Phil Alden Robinson, is a spin-off and sequel to The Good Wife, which was created by the Kings. The first season premiered on February 19, 2017, with the first episode airing on CBS and the following episodes on CBS All Access.On July 20, 2021, Paramount+ renewed the series for a sixth season. On May 27, 2022, it was announced that the sixth season will be the series' last;  it premiered on September 8, 2022.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = wikipedia.summary(query, sentences=5)\n",
    "    print(results)\n",
    "except:\n",
    "    print('fiale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794242f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e318cd",
   "metadata": {},
   "source": [
    "# `3) using source code (src)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b26bd",
   "metadata": {},
   "source": [
    "--> **implementing with scripts written in src folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83016f1b",
   "metadata": {},
   "source": [
    "### `3.1) src implementation` - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f6a4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.NLU.preprocess import prepare_data, get_tfidf, encode_label, split\n",
    "from src.NLU.intention import IntentRecognizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1601b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_df = pd.read_csv('Data/fluent_speech_commands_dataset/data/train_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e504293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modaj\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\src\\NLU\\preprocess.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"transcription_clean\"] = data[\"transcription\"].apply(lambda x: text_clean(x))\n"
     ]
    }
   ],
   "source": [
    "fs_df_clean = prepare_data(fs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80cf53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, tfidf_vectorizer = get_tfidf(fs_df_clean[\"transcription_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676b6f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': LabelEncoder(), 'object': LabelEncoder(), 'location': LabelEncoder()}\n",
      "{'action': array([2, 0, 0, ..., 2, 2, 3]), 'object': array([10,  8,  7, ...,  3,  1,  7]), 'location': array([2, 2, 2, ..., 2, 2, 3])}\n"
     ]
    }
   ],
   "source": [
    "les = {}\n",
    "ys = {}\n",
    "ys[\"action\"], les[\"action\"] = encode_label(fs_df_clean[\"action\"])\n",
    "ys[\"object\"], les[\"object\"] = encode_label(fs_df_clean[\"object\"])\n",
    "ys[\"location\"], les[\"location\"] = encode_label(fs_df_clean[\"location\"])\n",
    "\n",
    "print(les)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d2fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = IntentRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "036a894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent: action\n",
      "accuracy 1.0\n",
      "--------------------------------------------------\n",
      "intent: object\n",
      "accuracy 1.0\n",
      "--------------------------------------------------\n",
      "intent: location\n",
      "accuracy 1.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for intent, y_intent in ys.items():\n",
    "    print(\"intent: {}\".format(intent))\n",
    "    X_train, X_test, ys_train, ys_test = split(X, ys[intent], test_size=0.2, stratify=True)\n",
    "    \n",
    "    ir.learn_intents(X_train, ys_train, intent)\n",
    "    ir.evaluate(X_test, ys_test, intent)\n",
    "    print(\"-\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c1a571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': LogisticRegression(),\n",
       " 'object': LogisticRegression(),\n",
       " 'location': LogisticRegression()}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.intent_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c19248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'activate', 'object': 'lights', 'location': 'kitchen'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_eg = \"activate lights in kitchen\"\n",
    "ir.get_intents(transcript_eg, les, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91623fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save work\n",
    "with open('Saved/intention_models.pickle', 'wb') as handle:\n",
    "    pickle.dump(ir.intent_models, handle, protocol=4)  \n",
    "with open('Saved/tfidfV.pickle', 'wb') as handle:\n",
    "    pickle.dump(tfidf_vectorizer, handle, protocol=4)  \n",
    "with open('Saved/les.pickle', 'wb') as handle:\n",
    "    pickle.dump(les, handle, protocol=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ae233",
   "metadata": {},
   "source": [
    "### `3.2) src implementation` - usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49c7628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dialog_manager.manage import verify_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b2df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Saved/intention_models.pickle', 'rb') as handle:\n",
    "    intention_models_b = pickle.load(handle)\n",
    "with open('Saved/tfidfV.pickle', 'rb') as handle:\n",
    "    tfidfv_b = pickle.load(handle)\n",
    "with open('Saved/les.pickle', 'rb') as handle:\n",
    "    les_b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2e0042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_b = IntentRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dd2ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_b.intent_models = intention_models_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "949f8f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'activate', 'object': 'lights', 'location': 'washroom'}\n"
     ]
    }
   ],
   "source": [
    "transcript_eg = \"activate lights in washroom in home\"\n",
    "\n",
    "if \"in home\" in transcript_eg:\n",
    "    intents = ir_b.get_intents(transcript_eg, les_b, tfidfv_b)\n",
    "    ver_state, ver_intent = verify_intent(intents[\"action\"], intents[\"object\"], intents[\"location\"])\n",
    "    if not ver_state:\n",
    "        pass\n",
    "    else:\n",
    "        print(ver_intent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac6ac101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent verfied: True\n",
      "\n",
      "intents: {'action': 'activate', 'object': 'lights', 'location': 'washroom'}\n"
     ]
    }
   ],
   "source": [
    "ver_state, ver_intent = verify_intent(intents[\"action\"], intents[\"object\"], intents[\"location\"])\n",
    "print(\"intent verfied: {}\".format(ver_state))\n",
    "print()\n",
    "print(\"intents: {}\".format(ver_intent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0d3b823",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en'"
     ]
    }
   ],
   "source": [
    "import en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d710ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9b49a",
   "metadata": {},
   "source": [
    "## Further Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8c915",
   "metadata": {},
   "source": [
    "- https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85\n",
    "- https://igorizraylevych.medium.com/how-do-task-oriented-dialogue-systems-work-and-what-benefits-they-bring-for-business-20691bf2e0ae\n",
    "- https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926\n",
    "- https://medium.com/analytics-vidhya/a-guide-to-your-own-a-i-voice-assistant-using-python-17f79c94704"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f87b42",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/datasets/elvinagammed/chatbots-intent-recognition-dataset\n",
    "- https://www.kaggle.com/datasets/lorencpetr/chatbot-intent-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr-test-venv",
   "language": "python",
   "name": "sr-test-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
