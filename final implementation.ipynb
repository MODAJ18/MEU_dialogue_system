{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fff7bbd",
   "metadata": {},
   "source": [
    "## keywords for zero shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0734f76",
   "metadata": {},
   "source": [
    "- I, think -----> conversational"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a58106",
   "metadata": {},
   "source": [
    "# `1) Setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff210b2f",
   "metadata": {},
   "source": [
    "### `1.1) Speech Recognition - setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ade4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb19732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeCommand():\n",
    "    #It takes microphone input from the user and returns string output    \n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        r.pause_threshold = 1\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            print(\"Recognizing...\")    \n",
    "            query = r.recognize_google(audio, language='en-in') #Using google for voice recognition.\n",
    "            print(f\"User said: {query}\\n\")  #User query will be printed.    \n",
    "        except Exception as e:\n",
    "            # print(e)  use only if you want to print the error!\n",
    "            print(\"Say that again please...\")   #Say that again will be printed in case of improper voice \n",
    "            return \"None\" #None string will be returned\n",
    "        return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde55d1",
   "metadata": {},
   "source": [
    "### `1.2) NLU - setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa2edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.NLU.intention import IntentRecognizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b11ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "from parrot import Parrot\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29944051",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Saved/intention_models.pickle', 'rb') as handle:\n",
    "    intention_models_trained_b = pickle.load(handle)\n",
    "with open('Saved/tfidfV.pickle', 'rb') as handle:\n",
    "    tfidfv_b = pickle.load(handle)\n",
    "with open('Saved/les.pickle', 'rb') as handle:\n",
    "    les_b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc8aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_b = IntentRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841aca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use models trained\n",
    "ir_b.intent_models = intention_models_trained_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09054cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'deactivate', 'object': 'music', 'location': 'none'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_eg = \"turn off music\"\n",
    "ir_b.get_intents(transcript_eg, les_b, tfidfv_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43f12c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - zero shot model\n",
    "# zs_model = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "zs_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def get_discourse(query_in, labels_in):\n",
    "    zsc_result = zs_model(query_in, labels_in)\n",
    "    zsc_labels = zsc_result[\"labels\"]  \n",
    "    zsc_scores = zsc_result[\"scores\"]\n",
    "    discourse_prediction = zsc_labels[zsc_scores.index(max(zsc_scores))]\n",
    "    return discourse_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f86add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - conversational\n",
    "converser = pipeline(\"conversational\",\n",
    "                      model=\"facebook/blenderbot-400M-distill\")\n",
    "# query_test = \"I like going to shops\"\n",
    "# conversation = Conversation(query_test)\n",
    "# response = converser(conversation).generated_responses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d1ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- paraphrasing\n",
    "# paraphraser = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n",
    "\n",
    "# phrases = [\"Can you recommed some upscale restaurants in Newyork?\",\n",
    "#            \"What are the famous places we should not miss in Russia?\"\n",
    "# ]\n",
    "\n",
    "# for phrase in phrases:\n",
    "#     print(\"-\"*100)\n",
    "#     print(\"Input_phrase: \", phrase)\n",
    "#     print(\"-\"*100)\n",
    "#     para_phrases = paraphraser.augment(input_phrase=phrase)\n",
    "#     for para_phrase in para_phrases:\n",
    "#         print(para_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe4d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - storytelling\n",
    "# from transformers import pipeline\n",
    "\n",
    "# story_gen = pipeline(\"text-generation\", \"pranavpsv/gpt2-genre-story-generator\")\n",
    "# print(story_gen(\"<BOS> <superhero> Batman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b033b88",
   "metadata": {},
   "source": [
    "### `1.3) Dialog Management and State Tracking - setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7469fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dialog_manager.manage import manage_dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11dc3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = {\"music\": {\"none\": \"off\"},\n",
    "          \"lights\": {\"none\": \"on\", \"bedroom\": \"on\", \"washroom\":\"on\", \"kitchen\": \"on\"},\n",
    "          \"volume\": {\"none\":\"3\"},\n",
    "          \"heat\": {\"none\":\"warm\", \"bedroom\":\"warm\", \"washroom\":\"warm\", \"kitchen\": \"warm\"},\n",
    "          \"lamp\": {\"none\": \"off\"},\n",
    "          \"newspaper\": {\"none\": \"not brought\"},\n",
    "          \"juice\": {\"none\": \"not brought\"},\n",
    "          \"socks\": {\"none\": \"not brought\"},\n",
    "          \"chinese\": {\"none\": \"off\"},\n",
    "          \"korean\": {\"none\": \"off\"},\n",
    "          \"english\": {\"none\": \"on\"},\n",
    "          \"german\": {\"none\": \"off\"},\n",
    "          \"shoes\": {\"none\": \"not brought\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3642944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = initial_states\n",
    "action_prompt, object_prompt, volume_prompt = 'increase', 'volume', 'none'\n",
    "current_intent_test = {'action': 'increase', 'object': 'volume', 'location': 'none'}\n",
    "states_main, prompt_main, prev_states_main = manage_dialog(current_intent_test, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b5c0a",
   "metadata": {},
   "source": [
    "### `1.4) Text-to-Speech - setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b4b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "682f3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the voice\n",
    "engine = pyttsx3.init('sapi5')\n",
    "voices = engine.getProperty('voices') #gets you the details of the current voice\n",
    "\n",
    "engine.setProperty('voice', voices[1].id)  # 0-male voice , 1-female voice\n",
    "engine.setProperty('rate', 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "051468e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(audio):   \n",
    "    engine.say(audio)    \n",
    "    engine.runAndWait()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29f59161",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cdc2f",
   "metadata": {},
   "source": [
    "### `1.5) NLG - setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b430eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet_time():\n",
    "    hour = int(datetime.datetime.now().hour)\n",
    "    if hour>=0 and hour<12:\n",
    "        speak(\"Good Morning!\")    \n",
    "    \n",
    "    elif hour>=12 and hour<18:\n",
    "        speak(\"Good Afternoon!\")       \n",
    "    \n",
    "    else:\n",
    "        speak(\"Good Evening!\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912aec3",
   "metadata": {},
   "source": [
    "### `1.6) Other Libraries - setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7825902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import pyttsx3\n",
    "import datetime\n",
    "import wikipedia\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854adc2a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e350861",
   "metadata": {},
   "source": [
    "# `2) Running`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85a4ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_labels = [\"conversational\", \"command\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c550ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recognizing...\n",
      "result2:\n",
      "{   'alternative': [{'confidence': 0.97219545, 'transcript': 'open YouTube'}],\n",
      "    'final': True}\n",
      "User said: open YouTube\n",
      "\n",
      "QUERY: open youtube\n",
      "command\n",
      "Listening...\n",
      "Recognizing...\n",
      "result2:\n",
      "[]\n",
      "Say that again please...\n",
      "QUERY: none\n",
      "command\n",
      "Listening...\n",
      "Recognizing...\n",
      "result2:\n",
      "[]\n",
      "Say that again please...\n",
      "QUERY: none\n",
      "command\n",
      "Listening...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m prev_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mtakeCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;66;03m#Converting user query into lower case\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUERY: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(query))\n\u001b[0;32m      7\u001b[0m     discourse_prediction \u001b[38;5;241m=\u001b[39m get_discourse(query_in\u001b[38;5;241m=\u001b[39mquery, labels_in\u001b[38;5;241m=\u001b[39mdiscourse_labels)\n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36mtakeCommand\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mListening...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m r\u001b[38;5;241m.\u001b[39mpause_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 7\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecognizing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\test-venv\\lib\\site-packages\\speech_recognition\\__init__.py:677\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistening timed out while waiting for phrase to start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 677\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    679\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\test-venv\\lib\\site-packages\\speech_recognition\\__init__.py:211\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Personal\\Uni\\NLP\\project\\test-venv\\lib\\site-packages\\pyaudio.py:612\u001b[0m, in \u001b[0;36mStream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    610\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "greet_time()\n",
    "prev_response = \"\"\n",
    "\n",
    "while True:\n",
    "    query = takeCommand().lower() #Converting user query into lower case\n",
    "    print(\"QUERY: {}\".format(query))\n",
    "    discourse_prediction = get_discourse(query_in=query, labels_in=discourse_labels)\n",
    "    print(discourse_prediction)\n",
    "    \n",
    "    \n",
    "    if query == \"none\" or len(query) == 0:\n",
    "        continue\n",
    "    \n",
    "    if discourse_prediction == \"command\":\n",
    "        # Logic for executing tasks based on query\n",
    "        if 'wikipedia' in query:  #if wikipedia found in the query then this block will be executed\n",
    "            speak('Searching Wikipedia...')\n",
    "            query = query.replace(\"wikipedia\", \"\")\n",
    "            if query == \"\":\n",
    "                query = \"nothing\"\n",
    "            else:\n",
    "                query = \"what is life?\"\n",
    "            results = wikipedia.summary(query, sentences=5)\n",
    "            print(results)\n",
    "            response = \"According to Wikipedia, {}\".format(results)\n",
    "            speak(response)\n",
    "\n",
    "        elif 'open youtube' in query:\n",
    "            webbrowser.open(\"youtube.com\")\n",
    "            response = \"youtube opened\"\n",
    "            speak(response)\n",
    "\n",
    "        elif 'open google' in query:\n",
    "            webbrowser.open(\"google.com\")\n",
    "            response = \"google opened\"\n",
    "            speak(response)\n",
    "\n",
    "        elif 'play music' in query:\n",
    "            music_dir = 'music_dir_of_the_user'\n",
    "            songs = os.listdir(music_dir)\n",
    "            print(songs)    \n",
    "            os.startfile(os.path.join(music_dir, songs[0]))\n",
    "            response = \"playing music\"\n",
    "\n",
    "        elif 'the time' in query:\n",
    "            strTime = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "            response = f\"Sir, the time is {strTime}\"\n",
    "            speak(response)\n",
    "\n",
    "        elif ('open stackoverflow' in query) or ('open stack overflow' in query):                           \n",
    "            webbrowser.open('stackoverflow.com')\n",
    "            response = \"stackoverflow opened\"\n",
    "\n",
    "        elif ('open free code camp' in query) or ('open freecodecamp' in query):            \n",
    "            webbrowser.open('freecodecamp.org')\n",
    "            response = \"freecodecamp opened\"\n",
    "            \n",
    "        else:\n",
    "            response = \"sorry I didn't understand your command, can you repeat that please?\"\n",
    "            speak(response)\n",
    "            \n",
    "    \n",
    "    elif discourse_prediction == \"conversational\":\n",
    "        conversation = Conversation(query)\n",
    "        response = converser(conversation).generated_responses[-1]\n",
    "        speak(response)\n",
    "    \n",
    "    prev_response = response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr-test-venv",
   "language": "python",
   "name": "sr-test-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
